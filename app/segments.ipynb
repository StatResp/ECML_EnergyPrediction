{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "danish-december",
   "metadata": {},
   "source": [
    "# Segments\n",
    "\n",
    "This notebook creates the trip segments outlined in Section 3 of the paper from GTFS files. \n",
    "\n",
    "Outline:\n",
    "* Takes static GTFS schedules and generates trip segments.\n",
    "* Maps Inrix, HERE and OSM to the segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtfs_functions as gtfs\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import datetime as dt\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "from shapely.geometry import Point, LineString, Polygon, asShape, mapping\n",
    "import requests\n",
    "from plotly import graph_objs as go\n",
    "import numpy as np\n",
    "from shapely.ops import cascaded_union\n",
    "import folium\n",
    "import math\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import json\n",
    "import pymongo\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "os.chdir(\"../\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "GTFS_FILES = ['2019-08-18', '2020-04-13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "BUF = 20\n",
    "MAX_TRAJ_DIFF = 5\n",
    "MIN_NUM = 3\n",
    "INTERPOLATE_DIST = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file paths\n",
    "\n",
    "GTFS_UNZIPPED_DIR = os.path.join(os.getcwd(), 'data', 'gtfs', 'schedules_unzipped')\n",
    "GTFS_ZIPPED_DIR = os.path.join(os.getcwd(), 'data', 'gtfs', 'schedules_zipped')\n",
    "INRIX_SHAPES_PATH = os.path.join(os.getcwd(), 'data', 'inrix', 'USA_Tennessee.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file path\n",
    "\n",
    "SEGMENTS_PATH = os.path.join(os.getcwd(), 'output_r', 'segments', 'segments.pkl')\n",
    "TEMP_PATH = os.path.join(os.getcwd(), 'output_r', 'temp', 'segments.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-morris",
   "metadata": {},
   "source": [
    "# 1. Generate the Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gtfs(gtfs_filename, include_calendar=False):\n",
    "    dir_path = os.path.join(GTFS_UNZIPPED_DIR, gtfs_filename)\n",
    "    output_path = os.path.join(GTFS_ZIPPED_DIR, gtfs_filename)\n",
    "    shutil.make_archive(output_path, 'zip', dir_path)\n",
    "    \n",
    "    file_path = os.path.join(GTFS_ZIPPED_DIR, f\"{gtfs_filename}.zip\")\n",
    "    routes, stops, stop_times, trips, shapes = gtfs.import_gtfs(file_path, busiest_date=False)\n",
    "    routes = routes.drop_duplicates()\n",
    "    stops = stops.drop_duplicates()\n",
    "    stop_times = stop_times.drop_duplicates()\n",
    "    trips = trips.drop_duplicates()\n",
    "    shapes = shapes.drop_duplicates()\n",
    "    if include_calendar:\n",
    "        file_path = os.path.join(GTFS_UNZIPPED_DIR, gtfs_filename, 'calendar.txt')\n",
    "        calendar = pd.read_csv(file_path)\n",
    "        calendar['start_date'] = calendar['start_date'].apply(lambda x: dt.datetime.strptime(str(x), '%Y%m%d').date().isoformat())\n",
    "        calendar['end_date'] = calendar['end_date'].apply(lambda x: dt.datetime.strptime(str(x), '%Y%m%d').date().isoformat())\n",
    "        calendar = calendar[['service_id', 'start_date', 'end_date']]\n",
    "        return routes, stops, stop_times, trips, shapes, calendar\n",
    "    else:\n",
    "        return routes, stops, stop_times, trips, shapes\n",
    "\n",
    "\n",
    "def segments_per_trip(trip_id, stop_times):\n",
    "    result = {'trip_id': [], \n",
    "              'segment_seq': [],\n",
    "              'start_stop_id': [], \n",
    "              'start_stop_geometry': [], \n",
    "              'end_stop_id': [], \n",
    "              'end_stop_geometry': [], \n",
    "              'shape_id': [], \n",
    "              'direction_id': [], \n",
    "              'route_id': [], \n",
    "              'distance_btw_stops': []}\n",
    "    stop_times_trip = stop_times[stop_times['trip_id']==trip_id].sort_values(by=['stop_sequence']).reset_index()\n",
    "    for i in range(len(stop_times_trip)-1):\n",
    "        start_stop = stop_times_trip.iloc[i]\n",
    "        end_stop = stop_times_trip.iloc[i+1]\n",
    "        \n",
    "        result['trip_id'].append(trip_id)\n",
    "        result['segment_seq'].append(i)\n",
    "        result['start_stop_id'].append(start_stop['stop_id'])\n",
    "        result['start_stop_geometry'].append(start_stop['geometry'])\n",
    "        result['end_stop_id'].append(end_stop['stop_id'])\n",
    "        result['end_stop_geometry'].append(end_stop['geometry'])\n",
    "        result['shape_id'].append(start_stop['shape_id'])\n",
    "        result['direction_id'].append(start_stop['direction_id'])\n",
    "        result['route_id'].append(start_stop['route_id'])\n",
    "        distance_btw_stops = end_stop['shape_dist_traveled'] - start_stop['shape_dist_traveled']\n",
    "        result['distance_btw_stops'].append(distance_btw_stops)\n",
    "    return pd.DataFrame(result)\n",
    "        \n",
    "\n",
    "def generate_trip_segments(gtfs_filename):\n",
    "    routes, stops, stop_times, trips, shapes, calendar = load_gtfs(gtfs_filename, include_calendar=True)\n",
    "    segments_gdf = gtfs.cut_gtfs(stop_times, stops, shapes)\n",
    "    segments_gdf = segments_gdf.drop_duplicates(subset=['route_id', 'direction_id', 'shape_id', 'start_stop_id', 'end_stop_id'])\n",
    "    \n",
    "    df_list = []\n",
    "    for trip_id in stop_times['trip_id'].unique():\n",
    "        df = segments_per_trip(trip_id, stop_times)\n",
    "        df_list.append(df)\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    trip_segments = pd.merge(df, segments_gdf, on=['route_id', 'direction_id', 'shape_id', 'start_stop_id', 'end_stop_id'], suffixes=('','_segments_gdf'), how='left', validate='many_to_one')\n",
    "    temp = trip_segments['trip_id'].apply(lambda x: get_service_range(x, trips, calendar))\n",
    "    trip_segments['gtfs_start_date'] = [x[0] for x in temp]\n",
    "    trip_segments['gtfs_end_date'] = [x[1] for x in temp]\n",
    "    trip_segments['gtfs_filename'] = gtfs_filename\n",
    "    return trip_segments\n",
    "\n",
    "def get_service_range(trip_id, trips, calendar):\n",
    "    service_id = int(trips[trips['trip_id']==trip_id].iloc[0]['service_id'])\n",
    "    temp = calendar[calendar['service_id']==service_id].iloc[0]\n",
    "    return temp['start_date'], temp['end_date']\n",
    "\n",
    "\n",
    "def get_point(row, pref):\n",
    "    return Point(row[f\"{pref}_stop_lon\"], row[f\"{pref}_stop_lat\"])\n",
    "\n",
    "\n",
    "def format_segments(df, add_stop_points=True):\n",
    "    df = df.set_geometry('geometry')\n",
    "    df['trip_id'] = df['trip_id'].astype(int)\n",
    "    df['segment_seq'] = df['segment_seq'].astype(int)\n",
    "    df['start_stop_id'] = df['start_stop_id'].astype(int)\n",
    "    df['end_stop_id'] = df['end_stop_id'].astype(int)\n",
    "    df['direction_id'] = df['direction_id'].astype(int)\n",
    "    df['route_id'] = df['route_id'].astype(str)\n",
    "    df['distance_btw_stops'] = df['distance_btw_stops'].astype(float)\n",
    "    df['start_stop_name'] = df['start_stop_name'].astype(str)\n",
    "    df['end_stop_name'] = df['end_stop_name'].astype(str)\n",
    "    df['distance_m'] = df['distance_m'].astype(float)\n",
    "    df['gtfs_start_date'] = df['gtfs_start_date'].apply(lambda x: datetime.date.fromisoformat(x))\n",
    "    df['gtfs_end_date'] = df['gtfs_end_date'].apply(lambda x: datetime.date.fromisoformat(x))\n",
    "    df['segment_id'] = df['segment_id'].astype(str)\n",
    "\n",
    "    if add_stop_points:\n",
    "        df['start_stop_geometry'] = df.apply(lambda row: get_point(row, 'start'), axis=1)\n",
    "        df['end_stop_geometry'] = df.apply(lambda row: get_point(row, 'end'), axis=1)\n",
    "        df = df.drop(columns=['start_stop_lon', 'start_stop_lat', 'end_stop_lat', 'end_stop_lon'])\n",
    "    else:\n",
    "        df['start_stop_lon'] = df['start_stop_lon'].astype(float)\n",
    "        df['end_stop_lon'] = df['end_stop_lon'].astype(float)\n",
    "        df['start_stop_lat'] = df['start_stop_lat'].astype(float)\n",
    "        df['end_stop_lat'] = df['end_stop_lat'].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_segments(df1, df2, gtfs_end_date):\n",
    "    df_1 = deepcopy(df1)\n",
    "    df_2 = deepcopy(df2)\n",
    "    trip_ids = df_2['trip_id'].unique()\n",
    "    for trip_id in trip_ids:\n",
    "        temp1 = df_1[df_1['trip_id']==trip_id].drop(columns=['gtfs_start_date', 'gtfs_end_date']).sort_values(by=['segment_seq']).reset_index(drop=True)\n",
    "        temp2 = df_2[df_2['trip_id']==trip_id].drop(columns=['gtfs_start_date', 'gtfs_end_date']).sort_values(by=['segment_seq']).reset_index(drop=True)\n",
    "        if (len(temp1) > 0) & (temp1.equals(temp2)):\n",
    "            df_1.loc[(df_1['trip_id']==trip_id), 'gtfs_end_date'] = gtfs_end_date\n",
    "        elif (len(temp1) == 0):\n",
    "            temp = df_2[df_2['trip_id']==trip_id]\n",
    "            df_1 = pd.concat([df_1, temp], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"trip_id: {trip_id} is in df1 but is different when gtfs_end_date: {gtfs_end_date}\")\n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all segments from the GTFS files\n",
    "\n",
    "trip_segments_list = []\n",
    "for gtfs_name in GTFS_FILES:\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting gtfs: {gtfs_name}\")\n",
    "    temp = generate_trip_segments(gtfs_name)\n",
    "    trip_segments_list.append(temp)\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"Done with gtfs: {gtfs_name}, run took {end_time} seconds\")\n",
    "\n",
    "trip_segments = pd.concat(trip_segments_list, ignore_index=True)\n",
    "\n",
    "trip_segments['start_stop_lon'] = trip_segments['start_stop_geometry'].apply(lambda x: x.x)\n",
    "trip_segments['start_stop_lat'] = trip_segments['start_stop_geometry'].apply(lambda x: x.y)\n",
    "trip_segments['end_stop_lon'] = trip_segments['end_stop_geometry'].apply(lambda x: x.x)\n",
    "trip_segments['end_stop_lat'] = trip_segments['end_stop_geometry'].apply(lambda x: x.y)\n",
    "trip_segments = trip_segments.drop(columns=['start_stop_geometry', 'end_stop_geometry', 'stop_sequence', 'gtfs_filename'])\n",
    "\n",
    "t = gpd.GeoDataFrame(trip_segments, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge segments that appear in multiple GTFS files\n",
    "\n",
    "df = format_segments(t, add_stop_points=False)\n",
    "gtfs_groups = df.groupby(['gtfs_start_date','gtfs_end_date']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['gtfs_start_date'])\n",
    "df1 = df[(df['gtfs_start_date']==gtfs_groups.iloc[0]['gtfs_start_date']) & (df['gtfs_end_date']==gtfs_groups.iloc[0]['gtfs_end_date'])]\n",
    "for i in range(1, len(gtfs_groups)):\n",
    "    df2 = df[(df['gtfs_start_date']==gtfs_groups.iloc[i]['gtfs_start_date']) & (df['gtfs_end_date']==gtfs_groups.iloc[i]['gtfs_end_date'])]\n",
    "    df1 = merge_segments(df1, df2, gtfs_groups.iloc[i]['gtfs_end_date'])\n",
    "\n",
    "df1['gtfs_start_date'] = df1['gtfs_start_date'].apply(lambda x: x.isoformat())\n",
    "df1['gtfs_end_date'] = df1['gtfs_end_date'].apply(lambda x: x.isoformat())\n",
    "DF = gpd.GeoDataFrame(df1, geometry='geometry')\n",
    "\n",
    "DF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-consideration",
   "metadata": {},
   "source": [
    "# Add Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_elevation(lat,\n",
    "                      lon,\n",
    "                      units='Meters',\n",
    "                      max_tries=10,\n",
    "                      sec_btw_tries=1):\n",
    "    usgs_url = r'https://nationalmap.gov/epqs/pqs.php?'\n",
    "    usgs_params = {'output': 'json', 'x': lon, 'y': lat, 'units': units}\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "            usgs_request = requests.get(url=usgs_url,\n",
    "                                        params=usgs_params)\n",
    "            elevation = float(usgs_request.json()['USGS_Elevation_Point_Query_Service']['Elevation_Query']['Elevation'])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            elevation = None\n",
    "            time.sleep(sec_btw_tries)\n",
    "    return elevation\n",
    "\n",
    "\n",
    "def linestring_elevation_list(geom):\n",
    "    coords = list(geom.coords)\n",
    "    elevations = []\n",
    "    for p in coords:\n",
    "        lon, lat = p[0], p[1]\n",
    "        elevation = request_elevation(lat, lon)\n",
    "        if elevation is not None:\n",
    "            elevations.append(elevation)\n",
    "    return elevations\n",
    "\n",
    "\n",
    "def apply_elevation(geom, segment_id):\n",
    "    elevations = linestring_elevation_list(geom)\n",
    "    return {'segment_id': segment_id, 'elevation_list': elevations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_segment_ids = list(DF['segment_id'].unique())\n",
    "print(f\"There are {len(unique_segment_ids)} unique segments to get elevation for\")\n",
    "results = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:\n",
    "    future_list = []\n",
    "    for segment_id in unique_segment_ids:\n",
    "        temp = DF[DF['segment_id']==segment_id].sort_values(by=['distance_m'], ascending=False).iloc[0]\n",
    "        future_list.append(executor.submit(apply_elevation, temp['geometry'], segment_id))\n",
    "    for future in concurrent.futures.as_completed(future_list):\n",
    "        result = future.result()\n",
    "        results.append(result)\n",
    "        if (len(results) % 1000) == 0:\n",
    "            print(len(results))\n",
    "\n",
    "df_el = pd.DataFrame.from_records(results)\n",
    "df_el['maximum_elevation'] = df_el['elevation_list'].apply(lambda x: max(x))\n",
    "df_el['minimum_elevation'] = df_el['elevation_list'].apply(lambda x: min(x))\n",
    "df_el['start_elevation'] = df_el['elevation_list'].apply(lambda x: x[0])\n",
    "df_el['end_elevation'] = df_el['elevation_list'].apply(lambda x: x[-1])\n",
    "df_el['elevation_diff'] = df_el.apply(lambda row: row['maximum_elevation'] - row['minimum_elevation'], axis=1)\n",
    "df_el['elevation_change'] = df_el.apply(lambda row: row['start_elevation'] - row['end_elevation'], axis=1)\n",
    "df_el.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join DF and df_el\n",
    "\n",
    "df = pd.merge(left=DF, right=df_el, how='left', on='segment_id', validate='many_to_one')\n",
    "DF = df.copy(deep=True)\n",
    "DF.to_pickle(TEMP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-friday",
   "metadata": {},
   "source": [
    "# INRIX XD Segments - start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_pickle(TEMP_PATH)\n",
    "DF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory(point1, point2):\n",
    "    if isinstance(point1, Point):\n",
    "        traj = math.atan2(point2.y-point1.y, point2.x-point1.x)\n",
    "    else:\n",
    "        traj = math.atan2(point2[1]-point1[1], point2[0]-point1[0])\n",
    "    return math.degrees(traj)\n",
    "\n",
    "\n",
    "def traj_dif(traj1, traj2):\n",
    "    a = traj1 - traj2\n",
    "    if a > 180:\n",
    "        a -= 360\n",
    "    elif a < -180:\n",
    "        a += 360\n",
    "    return abs(a)\n",
    "\n",
    "\n",
    "def interpolate_linestring(segment, dist=INTERPOLATE_DIST):\n",
    "    \"\"\"\n",
    "    segment: LineString\n",
    "    dist: integer, meters\n",
    "    returns: LineString\n",
    "    \"\"\"\n",
    "    try:\n",
    "        distance = 0\n",
    "        add_distance = meter_to_degree(dist)\n",
    "        result = []\n",
    "        while distance < segment.length:\n",
    "            result.append(segment.interpolate(distance))\n",
    "            distance += add_distance\n",
    "        return LineString(result)\n",
    "    except:\n",
    "        return segment\n",
    "    \n",
    "    \n",
    "def meter_to_degree(buf):\n",
    "    \"\"\"\n",
    "    buf: integer, represents buffer in meters\n",
    "    returns: degrees\n",
    "    \"\"\"\n",
    "    return buf / 100000\n",
    "\n",
    "\n",
    "def set_tmc_buffer(df_tmc, buf=BUF):\n",
    "    \"\"\"\n",
    "    df_tmc: geopandas dataframe of tmc shape data\n",
    "    buf: integer, represents buffer in meters\n",
    "    returns: df_tmc geopandas dataframe with geometry_buf set to buffered objects\n",
    "    \"\"\"\n",
    "    df_tmc['geometry_buf'] = df_tmc['geometry'].buffer(meter_to_degree(buf))\n",
    "    df_tmc.set_geometry('geometry_buf', inplace=True)\n",
    "    return df_tmc\n",
    "\n",
    "\n",
    "def filter_tmcs(df_tmcs_mapped, point1, point2, max_traj_diff=MAX_TRAJ_DIFF):\n",
    "    result = []\n",
    "    if len(df_tmcs_mapped) > 0:\n",
    "        seg_traj = trajectory(point1, point2)\n",
    "        for k, v in df_tmcs_mapped.iterrows():\n",
    "            point1_proj = v['geometry'].project(point1)\n",
    "            point2_proj = v['geometry'].project(point2)\n",
    "            if point2_proj > point1_proj:\n",
    "                tmc_traj = trajectory(v['geometry'].interpolate(point1_proj), v['geometry'].interpolate(point2_proj))\n",
    "                if traj_dif(seg_traj, tmc_traj) < max_traj_diff:\n",
    "                    result.append(v['XDSegID'])\n",
    "    return result\n",
    "\n",
    "\n",
    "def map_segment(df_tmc, segment, min_num=MIN_NUM):\n",
    "    tmc_id_list = []\n",
    "    full_segment = interpolate_linestring(segment)\n",
    "    full_segment_coords = list(full_segment.coords)\n",
    "    for i in range(len(full_segment_coords)-1):\n",
    "        point1, point2 = Point(full_segment_coords[i]), Point(full_segment_coords[i+1])\n",
    "        seg = LineString([point1, point2])\n",
    "        df_tmcs_mapped = df_tmc.loc[df_tmc['geometry_buf'].contains(seg)]\n",
    "        #df_tmcs_mapped = bounding_tmc_ids(df_tmc, seg)\n",
    "        tmc_ids = filter_tmcs(df_tmcs_mapped, point1, point2)\n",
    "        tmc_id_list += tmc_ids\n",
    "    result = set()\n",
    "    for tmc_id in tmc_id_list:\n",
    "        if tmc_id_list.count(tmc_id) >= min_num:\n",
    "            result.add(tmc_id)\n",
    "    return list(result)\n",
    "\n",
    "\n",
    "def add_tmcs_to_gtfs_shapes(df_inrix, df_segments):\n",
    "    df_tmc = deepcopy(df_inrix)\n",
    "    #df_tmc = set_tmc_buffer(df_inrix)\n",
    "    result = {'segment_id': [], 'XDSegID': [], 'segment_geometry': []}\n",
    "    unique_segment_ids = df_segments['segment_id'].unique()\n",
    "    print(f\"There are {len(unique_segment_ids)} unique segments to map\")\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    for segment_id in unique_segment_ids:\n",
    "        v = df_segments[df_segments['segment_id']==segment_id].sort_values(by=['distance_m'], ascending=False).iloc[0]\n",
    "        tmc_id = map_segment(df_tmc, v['geometry'])\n",
    "        result['segment_id'].append(segment_id)\n",
    "        result['segment_geometry'].append(v['geometry'])\n",
    "        if len(tmc_id) > 0:\n",
    "            result['XDSegID'].append(tmc_id)\n",
    "        else:\n",
    "            result['XDSegID'].append(None)\n",
    "        if (counter % 100) == 0:\n",
    "            end_time = time.time() - start_time\n",
    "            print(f\"Done with {counter} segments. Took {end_time} seconds\")\n",
    "            start_time = time.time()\n",
    "        counter += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load segments\n",
    "\n",
    "df_segments = DF.copy(deep=True)\n",
    "\n",
    "# load inrix shapes\n",
    "\n",
    "with open(INRIX_SHAPES_PATH) as f:\n",
    "    df_inrix = gpd.read_file(f)\n",
    "    df_inrix = df_inrix.set_geometry('geometry')\n",
    "    \n",
    "print(f\"number of shapes in all of tennessee: {len(df_inrix)}\")\n",
    "bounds = df_segments['geometry'].total_bounds\n",
    "bounding_square = Polygon([(bounds[0], bounds[1]), (bounds[0], bounds[3]), (bounds[2], bounds[3]), (bounds[2], bounds[1])])\n",
    "df_inrix = df_inrix[df_inrix['geometry'].intersects(bounding_square)]\n",
    "print(f\"number of shapes in chattanooga region: {len(df_inrix)}\")\n",
    "#df_inrix.head(2)\n",
    "\n",
    "# process\n",
    "\n",
    "df_inrix = set_tmc_buffer(df_inrix, buf=BUF)\n",
    "result = add_tmcs_to_gtfs_shapes(df_inrix, df_segments)\n",
    "\n",
    "df_result = pd.DataFrame.from_records(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_result[['XDSegID', 'segment_id']]\n",
    "df = pd.merge(left=DF, right=df_result, how='left', on='segment_id', validate='many_to_one')\n",
    "DF = df.copy(deep=True)\n",
    "DF['XDSegID'] = DF['XDSegID'].apply(lambda x: format_xdseg(x))\n",
    "DF.to_pickle(TEMP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-herald",
   "metadata": {},
   "source": [
    "# OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_seg_col(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace(\"[\", \"\")\n",
    "        x = x.replace(\"]\", \"\")\n",
    "        x = x.replace(\"'\", \"\")\n",
    "        return [int(y) for y in x.split(\",\")]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def format_col(x):\n",
    "    if isinstance(x, str):\n",
    "        return [int(y) for y in x.split(\";\")]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def get_osm_ways(x, df_xd_osm_map):\n",
    "    result = []\n",
    "    if isinstance(x, list):\n",
    "        for xd in x:\n",
    "            way = df_xd_osm_map[df_xd_osm_map['XDSegID']==xd]\n",
    "            if len(way) == 1:\n",
    "                result.append(way.iloc[0]['OSMWayIDs'])\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        return x\n",
    "    if len(result) > 0:\n",
    "        return list(set([item for sublist in result for item in sublist]))\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_osm_fclasses(x, df_osm):\n",
    "    result = []\n",
    "    if isinstance(x, list):\n",
    "        for osm_way in x:\n",
    "            fclass = df_osm[df_osm['osm_id']==osm_way]\n",
    "            if len(fclass) == 1:\n",
    "                result.append(fclass.iloc[0]['fclass'])\n",
    "    else:\n",
    "        return x\n",
    "    return list(set(result))\n",
    "\n",
    "def format_xdseg(x):\n",
    "    try:\n",
    "        return [int(y) for y in x]\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xd = df_result.copy(deep=True)\n",
    "df_xd['XDSegID'] = df_xd['XDSegID'].apply(lambda x: format_xdseg(x))\n",
    "\n",
    "df_osm = gpd.read_file(os.path.join(os.getcwd(), 'data', 'inrix', 'osm', 'chattanooga_osm.shp'))\n",
    "df_osm['osm_id'] = df_osm['osm_id'].astype(int)\n",
    "\n",
    "df_xd_osm_map = pd.read_csv(os.path.join(os.getcwd(), 'data', 'inrix', 'xd_to_osm.csv'))[['XDSegID', 'OSMWayIDs']]\n",
    "df_xd_osm_map['OSMWayIDs'] = df_xd_osm_map['OSMWayIDs'].apply(lambda x: format_col(x))\n",
    "df_xd_osm_map['XDSegID'] = df_xd_osm_map['XDSegID'].astype(int)\n",
    "\n",
    "df_xd['osm_ways'] = df_xd['XDSegID'].apply(lambda x: get_osm_ways(x, df_xd_osm_map))\n",
    "\n",
    "df_xd['osm_way_fclasses'] = df_xd['osm_ways'].apply(lambda x: get_osm_fclasses(x, df_osm))\n",
    "\n",
    "DF = DF.drop(columns=['XDSegID'])\n",
    "DF = pd.merge(left=DF, right=df_xd, how='left', on='segment_id', validate='many_to_one')\n",
    "DF.to_pickle(TEMP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-listening",
   "metadata": {},
   "source": [
    "# HERE TMC Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_here_shapes(client, db_name='data-class', collection_name='here.chattanooga.shapes'):\n",
    "    df_tmc = gpd.GeoDataFrame(list(client[db_name][collection_name].find()))\n",
    "    df_tmc['geometry'] = df_tmc['geom'].apply(lambda x: flatten_geoms(x))\n",
    "    df_tmc.drop(columns=['_id'], inplace=True)\n",
    "    return df_tmc\n",
    "\n",
    "\n",
    "def get_mongo_connection():\n",
    "    with open(CONFIG_PATH) as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    mongo_url = \"mongodb://{}:{}@{}:{}/?authSource={}\".format(config[\"user\"],\n",
    "                                                              config[\"password\"],\n",
    "                                                              config[\"host\"],\n",
    "                                                              config[\"port\"],\n",
    "                                                              config[\"authenticationDatabase\"])\n",
    "\n",
    "    client = pymongo.MongoClient(mongo_url)\n",
    "    return client\n",
    "\n",
    "\n",
    "def flatten_geoms(x):\n",
    "    geom = x['coordinates']\n",
    "    if x['type'] == \"MultiLineString\":\n",
    "        result = []\n",
    "        for linestring in geom:\n",
    "            result += linestring\n",
    "    else:\n",
    "        result = geom\n",
    "    return LineString(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tmc shapes\n",
    "\n",
    "CONFIG_PATH = os.path.join(os.getcwd(), \"config\", \"config.json\")\n",
    "client = get_mongo_connection()\n",
    "df_tmc = query_here_shapes(client)\n",
    "client.close()\n",
    "\n",
    "# load segments\n",
    "df_segments = DF.copy(deep=True)\n",
    "\n",
    "\n",
    "# process\n",
    "df_tmc = set_tmc_buffer(df_tmc, buf=BUF)\n",
    "df_tmc['XDSegID'] = df_tmc['tmc_id']\n",
    "result = add_tmcs_to_gtfs_shapes(df_tmc, df_segments)\n",
    "\n",
    "df = pd.DataFrame.from_records(result)\n",
    "df['tmc_id'] = df['XDSegID']\n",
    "df = df.drop(columns=['XDSegID', 'segment_geometry'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.merge(left=DF, right=df, how='left', on='segment_id', validate='many_to_one')\n",
    "DF.to_pickle(SEGMENTS_PATH)\n",
    "DF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"length of segments: {len(DF)}, null osm ways: {DF['osm_way_fclasses'].isnull().sum()}, null XDSegIDs: {DF['XDSegID'].isnull().sum()}, null tmc ids: {DF['tmc_id'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"length of segments: {len(DF)}, null osm ways: {DF['osm_way_fclasses'].isnull().sum()}, null XDSegIDs: {DF['XDSegID'].isnull().sum()}, null tmc ids: {DF['tmc_id'].isnull().sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
