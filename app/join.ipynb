{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "robust-accounting",
   "metadata": {},
   "source": [
    "# Data Join\n",
    "\n",
    "Must generate segments with segments.ipynb first. This notebook joins our various data sources as discussed in Section 3 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gtfs_functions as gtfs\n",
    "#import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import datetime as dt\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "from shapely.geometry import Point, LineString, Polygon, asShape, mapping\n",
    "import requests\n",
    "from plotly import graph_objs as go\n",
    "import numpy as np\n",
    "from shapely.ops import cascaded_union, transform\n",
    "from functools import partial\n",
    "import pyproj\n",
    "#import folium\n",
    "import math\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import json\n",
    "#import pymongo\n",
    "import pytz \n",
    "#import dask.dataframe as dd\n",
    "#from dask.distributed import Client\n",
    "#from haversine import haversine, Unit\n",
    "#import swifter\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "os.chdir(\"../\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Settings\n",
    "\n",
    "JOIN_TOLERANCE = 2005\n",
    "SEG_LOOKAHEAD = 2\n",
    "BUFFER = 25\n",
    "TRAFFIC_BUFFER = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-manufacturer",
   "metadata": {},
   "source": [
    "## 1. Data Merge\n",
    "\n",
    "Joins raw ViriCiti, Clever and Weather data. Need to first generate the GTFS segments in segments.ipynb.\n",
    "\n",
    "Reads from: Mongo\n",
    "\n",
    "Writes to: output/data-joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = os.path.join(os.getcwd(), \"config\", \"config.json\")\n",
    "DB = 'data-class'\n",
    "VIRICITI_COLLECTION = 'viriciti.one.sec'\n",
    "CLEVER_COLLECTION = 'bustime.chattanooga'\n",
    "WEATHER_COLLECTION = 'darksky.chattanooga'\n",
    "TZ = pytz.timezone('EST')\n",
    "HOUR = 3600\n",
    "\n",
    "BYD_PARAMS = [\"soc\",\n",
    "              \"odo\",\n",
    "              \"speed\",\n",
    "              \"current\",\n",
    "              \"voltage\",\n",
    "              \"power\",\n",
    "              \"gps\"]\n",
    "\n",
    "\n",
    "GILLIG_PARAMS = [\"fuel\",\n",
    "                 \"odo\",\n",
    "                 \"speed\",\n",
    "                 \"gps\"]\n",
    "\n",
    "VEHICLE_FLEETS = {\"Diesel\": [str(x) for x in range(101, 151)],\n",
    "                  \"Hybrid\": [str(x) for x in range(501, 507)],\n",
    "                  \"Electric\": [str(x) for x in range(751, 754)]}\n",
    "\n",
    "VEHICLE_TYPES = {\"Gillig LF 35' 2014\": [str(x) for x in range(147, 151)],\n",
    "                 \"Gillig HF 2006\": [str(x) for x in range(135, 145)],\n",
    "                 \"Gillig HF 35' 2002\": [str(x) for x in range(111, 135)],\n",
    "                 \"Gillig HF 30' 1998\": [str(x) for x in range(101, 111)],\n",
    "                 \"Gillig 35' Hybrids 2014\": [str(x) for x in range(503, 507)],\n",
    "                 \"Gillig 37' Hybrids 2009\": [str(x) for x in range(501, 503)],\n",
    "                 \"Gillig LF 37' 2009\": [str(x) for x in range(145, 147)],\n",
    "                 \"BYD BEV\": [str(x) for x in range(751, 754)]}\n",
    "\n",
    "\n",
    "VALID_VEHICLE_TYPES = [\"Gillig LF 35' 2014\", \"Gillig 35' Hybrids 2014\", 'BYD BEV']\n",
    "INVALID_ROUTES = ['DH', 'PI', 'PO', 'U']\n",
    "DIESEL_PATH = os.path.join(os.getcwd(), 'output_r', 'data-joins', 'diesel')\n",
    "HYBRID_PATH = os.path.join(os.getcwd(), 'output_r', 'data-joins', 'hybrid')\n",
    "ELECTRIC_PATH = os.path.join(os.getcwd(), 'output_r', 'data-joins', 'electric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mongo_connection():\n",
    "    with open(CONFIG_PATH) as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    mongo_url = \"mongodb://{}:{}@{}:{}/?authSource={}\".format(config[\"user\"],\n",
    "                                                              config[\"password\"],\n",
    "                                                              config[\"host\"],\n",
    "                                                              config[\"port\"],\n",
    "                                                              config[\"authenticationDatabase\"])\n",
    "\n",
    "    client = pymongo.MongoClient(mongo_url)\n",
    "    return client\n",
    "\n",
    "\n",
    "def timestamp_to_datetime(timestamp, millis=False):\n",
    "    if millis is True:\n",
    "        ts = timestamp / 1000\n",
    "    else:\n",
    "        ts = timestamp\n",
    "    dt = datetime.datetime.fromtimestamp(ts, tz=TZ)\n",
    "    return dt\n",
    "\n",
    "\n",
    "def datestr_to_timestamp(datestr=\"2020/11/15\", millis=False):\n",
    "    # datestr in format month/day/year\n",
    "    year, month, day = [int(x) for x in datestr.split(\"/\")]\n",
    "    ts = datetime.datetime(year=year, month=month, day=day, hour=0, tzinfo=TZ).timestamp()\n",
    "    if millis is False:\n",
    "        return int(ts)\n",
    "    else:\n",
    "        ts = ts * 1000\n",
    "        return int(ts)\n",
    "    \n",
    "    \n",
    "def datetimestr_to_timestamp(datestr=\"11/11/2020-08:00\", millis=False):\n",
    "    # datestr in format month/day/year\n",
    "    d, t = datestr.split(\"-\")\n",
    "    month, day, year = [int(x) for x in d.split(\"/\")]\n",
    "    hour, minute = [int(x) for x in t.split(\":\")]\n",
    "    ts = datetime.datetime(year=year, month=month, day=day, hour=hour, minute=minute, tzinfo=TZ).timestamp()\n",
    "    if millis is False:\n",
    "        return int(ts)\n",
    "    else:\n",
    "        ts = ts * 1000\n",
    "        return int(ts)\n",
    "    \n",
    "    \n",
    "def query_trips_from_clever(client, start_timestamp, end_timestamp):\n",
    "    start_time = int(start_timestamp / 1000)\n",
    "    end_time = int(end_timestamp / 1000)\n",
    "    match = {\"$match\": {\"timestamp\": {\"$gte\": start_time, \"$lte\": end_time}, 'rt': {\"$nin\": INVALID_ROUTES}}}\n",
    "\n",
    "    pipeline = [match,\n",
    "                {'$sort': {'clever.timestamp': 1}},\n",
    "                {'$group': {'_id': {\"month\":\n",
    "                                        {'$month':\n",
    "                                             {\"$dateFromString\":\n",
    "                                                  {'dateString': '$tmstmp',\n",
    "                                                   'format': \"%Y%m%d %H:%M\"}}},\n",
    "                                    \"day\":\n",
    "                                        {'$dayOfMonth':\n",
    "                                             {\"$dateFromString\":\n",
    "                                                  {'dateString': '$tmstmp',\n",
    "                                                   'format': \"%Y%m%d %H:%M\"}}},\n",
    "                                    \"year\":\n",
    "                                        {'$year':\n",
    "                                             {\"$dateFromString\":\n",
    "                                                  {'dateString': '$tmstmp',\n",
    "                                                   'format': \"%Y%m%d %H:%M\"}}},\n",
    "                                    \"tripID\": '$tatripid'},\n",
    "                            'tID': {'$first': '$tatripid'},\n",
    "                            'vID': {'$first': '$vid'},\n",
    "                            'dCT': {'$sum': 1},\n",
    "                            'sEP': {'$first': '$timestamp'},\n",
    "                            'eEP': {'$last': '$timestamp'},\n",
    "                            'sTM': {'$first': '$tmstmp'},\n",
    "                            'sRT': {'$first': '$rt'},\n",
    "                            'eTM': {'$last': '$tmstmp'}}},\n",
    "                {'$replaceWith': {'clever_vid': '$vID',\n",
    "                                  'trip_id': '$tID',\n",
    "                                  'documents': '$dCT',\n",
    "                                  'start_timestamp': '$sEP',\n",
    "                                  'end_timestamp': '$eEP',\n",
    "                                  'start_time': '$sTM',\n",
    "                                  'end_time': '$eTM',\n",
    "                                  'rt': '$sRT',\n",
    "                                  'test': '$_id'}}]\n",
    "    result = list(client[DB][CLEVER_COLLECTION].aggregate(pipeline, allowDiskUse=True))\n",
    "    trips = pd.DataFrame(result)\n",
    "    if len(trips) > 0:\n",
    "        trips['start_datetime'] = trips['start_timestamp'].apply(lambda x: timestamp_to_datetime(x, millis=False))\n",
    "        trips['datetime'] = trips['start_timestamp'].apply(lambda x: timestamp_to_datetime(x, millis=False))\n",
    "        trips['end_datetime'] = trips['end_timestamp'].apply(lambda x: timestamp_to_datetime(x, millis=False))\n",
    "        trips['date'] = trips['start_datetime'].apply(lambda x: x.strftime(\"%Y/%m/%d\"))\n",
    "        trips['hour'] = trips['start_datetime'].apply(lambda x: int(x.strftime(\"%H\")))\n",
    "        trips['date-hour'] = trips.apply(lambda row: f\"{row['date']}-{int(row['hour'])}\", axis=1)\n",
    "        trips['fleet'] = trips['clever_vid'].apply(lambda x: get_fleet(x))\n",
    "        trips['vehicle_type'] = trips['clever_vid'].apply(lambda x: get_vehicle_type(x))\n",
    "        trips['start_timestamp'] = trips['start_timestamp'].apply(lambda x: x*1000)\n",
    "        trips['end_timestamp'] = trips['end_timestamp'].apply(lambda x: x*1000)\n",
    "        trips = trips.drop(['test', 'start_time', 'end_time'], axis=1)\n",
    "        trips = trips.drop_duplicates()\n",
    "        return trips\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_fleet(x):\n",
    "    for k, v in VEHICLE_FLEETS.items():\n",
    "        if x in v:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_vehicle_type(x):\n",
    "    for k, v in VEHICLE_TYPES.items():\n",
    "        if x in v:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "\n",
    "def fleet_to_parameters(fleet):\n",
    "    if fleet == 'Electric':\n",
    "        params_to_check = BYD_PARAMS\n",
    "    elif fleet == 'Diesel':\n",
    "        params_to_check = GILLIG_PARAMS\n",
    "    elif fleet == 'Hybrid':\n",
    "        params_to_check = GILLIG_PARAMS\n",
    "    else:\n",
    "        params_to_check = None\n",
    "    return params_to_check\n",
    "\n",
    "\n",
    "def original_query_viriciti(client, lower, upper, vid):\n",
    "    query = {\"clever_vid\": vid, 'time': {'$gte': lower, '$lte': upper}}\n",
    "    result = client[DB][VIRICITI_COLLECTION].find(query)\n",
    "    df = pd.DataFrame(result)\n",
    "    return df\n",
    "\n",
    "\n",
    "def query_viriciti(client, lower, upper, vid_list):\n",
    "    labels = ['analyses.fuel_used', 'gps', 'power']\n",
    "    #query = {'time': {'$gte': lower, '$lte': upper}, \"clever_vid\": {\"$in\": vid_list}}\n",
    "    query = {\"clever_vid\": {\"$in\": vid_list}, \"label\": {\"$in\": labels}, 'time': {'$lte': upper, '$gte': lower}}\n",
    "    result = client[DB][VIRICITI_COLLECTION].find(query)\n",
    "    df = pd.DataFrame(result)\n",
    "    return df\n",
    "\n",
    "\n",
    "def query_darksky(client, lower, upper):\n",
    "    match = {\"$match\": {\"time\": {\"$gte\": lower-HOUR, \"$lte\": upper+HOUR}}}\n",
    "    result = list(client[DB][WEATHER_COLLECTION].aggregate([match]))\n",
    "    df = pd.DataFrame(result)\n",
    "    df['time'] = df['time'].apply(lambda x: x*1000)\n",
    "    return df\n",
    "\n",
    "\n",
    "def join_weather(df, st_timestamp, et_timestamp):\n",
    "    df_weather = query_darksky(client, st_timestamp/1000, et_timestamp/1000)\n",
    "    df_weather = df_weather.sort_values(by=['time'])\n",
    "    df_result = df.sort_values(by=['time'])\n",
    "    df_result = pd.merge_asof(df_result, df_weather, on='time', suffixes=('', '_darksky'), direction='nearest')\n",
    "    df_result.drop(columns=['_id', 'geometry', 'timezone', 'agency', 'icon'])\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def get_label(df, label, base):\n",
    "    # extract label and perform label-name pivot        \n",
    "    df = df[df.label == label].sort_values(by=['time'])\n",
    "    df = df.drop(columns=['label']).rename(columns={\"value\": label})\n",
    "        \n",
    "    # drop columns for non-base labels\n",
    "    if not base:\n",
    "        df = df.drop(columns=['vehicle_type', 'trip_id', 'rt', 'vehicle_fleet', 'time_string'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def format_df_diesel_hybrid(df, tolerance=None, remove_nan_targets=False):\n",
    "    #df_left = get_label(df, 'gps', True)\n",
    "    #df_right = get_label(df, 'analyses.fuel_used', False)\n",
    "    df_left = get_label(df, 'analyses.fuel_used', True)\n",
    "    df_right = get_label(df, 'gps', False)\n",
    "    df_left  = pd.merge_asof(df_left, df_right, on='time', by='clever_vid', suffixes=('', '_gps'), direction='nearest', tolerance=tolerance)\n",
    "    if remove_nan_targets:\n",
    "        global NUM_SAMPLES\n",
    "        NUM_SAMPLES += len(df_left)\n",
    "        df_left = df_left.dropna(subset=['analyses.fuel_used', 'gps'])\n",
    "        global NUM_SAMPLES_MAPPED\n",
    "        NUM_SAMPLES_MAPPED += len(df_left)\n",
    "\n",
    "    return df_left\n",
    "\n",
    "\n",
    "def format_df_electric(df, tolerance=None, remove_nan_targets=False):\n",
    "    df_left = get_label(df, 'power', True)\n",
    "    df_right = get_label(df, 'gps', False)\n",
    "    df_left  = pd.merge_asof(df_left, df_right, on='time', by='clever_vid', suffixes=('', '_gps'), direction='nearest', tolerance=tolerance)\n",
    "    if remove_nan_targets:\n",
    "        global NUM_SAMPLES\n",
    "        NUM_SAMPLES += len(df_left)\n",
    "        df_left = df_left.dropna(subset=['power', 'gps'])\n",
    "        global NUM_SAMPLES_MAPPED\n",
    "        NUM_SAMPLES_MAPPED += len(df_left)\n",
    "    return df_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_run_day(st_datestr, et_datestr, tolerance=JOIN_TOLERANCE):\n",
    "    print(f\"starting {st_datestr} to {et_datestr}\")\n",
    "    start_time = time.time()\n",
    "    st_timestamp = datestr_to_timestamp(st_datestr, millis=True)\n",
    "    et_timestamp = datestr_to_timestamp(et_datestr, millis=True)\n",
    "    \n",
    "    # get trips from clever and unique vid in time window\n",
    "    df_clever = query_trips_from_clever(client, st_timestamp, et_timestamp)\n",
    "    if df_clever is None:\n",
    "        return None\n",
    "    df_clever = df_clever[df_clever['vehicle_type'].isin(VALID_VEHICLE_TYPES)]\n",
    "    vid_uniq = list(df_clever['clever_vid'].unique())\n",
    "\n",
    "    df_viriciti = query_viriciti(client, st_timestamp, et_timestamp, vid_uniq)\n",
    "    #df_viriciti = df_viriciti[df_viriciti['label'].isin(['analyses.fuel_used', 'gps', 'odo', 'speed', 'current', 'soc', 'voltage', 'power'])]\n",
    "    if len(df_viriciti) == 0:\n",
    "        return None\n",
    "    df_viriciti = df_viriciti[df_viriciti['label'].isin(['analyses.fuel_used', 'gps', 'power'])]\n",
    "    col_keep = list(df_viriciti.columns) + ['trip_id', 'rt']\n",
    "    \n",
    "    # process keys for cross join \n",
    "    df_clever = df_clever[['clever_vid', 'trip_id', 'start_timestamp', 'end_timestamp', 'rt']]\n",
    "    df_clever   = df_clever.assign(key=1)\n",
    "    df_viriciti = df_viriciti.assign(key=1)\n",
    "\n",
    "    # perform cross join on matching vid and query rows matching time criteria\n",
    "    df_merge = pd.merge(df_viriciti, df_clever, on = ['key', 'clever_vid'], suffixes=('','_clever')).drop('key',axis=1).query('time >= start_timestamp and time <= end_timestamp')\n",
    "    #df_merge = df_merge.query('time >= start_timestamp and time <= end_timestamp')\n",
    "\n",
    "    # get and drop unessential columns\n",
    "    col_drop = [c for c in list(df_merge.columns) if c not in col_keep]\n",
    "    df_merge = df_merge.drop(columns=col_drop)\n",
    "    \n",
    "    # add vehicle_fleet and vehicle_type\n",
    "    df_merge['vehicle_fleet'] = df_merge['clever_vid'].apply(lambda x: get_fleet(x))\n",
    "    df_merge['vehicle_type'] = df_merge['clever_vid'].apply(lambda x: get_vehicle_type(x))\n",
    "    df_merge = df_merge[['time', 'value', 'label', 'vehicle_type', 'vehicle_fleet', 'time_string', 'clever_vid', 'trip_id', 'rt']]\n",
    "    \n",
    "    # break into diesel, hybrid and electric\n",
    "    df_diesel = df_merge[df_merge['vehicle_fleet']=='Diesel']\n",
    "    df_hybrid = df_merge[df_merge['vehicle_fleet']=='Hybrid']\n",
    "    df_electric = df_merge[df_merge['vehicle_fleet']=='Electric']\n",
    "    #print(f\"Number of diesel: {len(df_diesel)}, number of hybrid: {len(df_hybrid)}, number of electric: {len(df_electric)}\")\n",
    "    \n",
    "    # diesel\n",
    "    try:\n",
    "        if len(df_diesel) > 0:\n",
    "            df = format_df_diesel_hybrid(df_diesel, tolerance=tolerance, remove_nan_targets=True)\n",
    "            if len(df_diesel) > 0:\n",
    "                df = join_weather(df, st_timestamp, et_timestamp)\n",
    "                temp = st_datestr.replace('/', '-')\n",
    "                out_path = os.path.join(DIESEL_PATH, f\"{temp}.csv\")\n",
    "                df.to_csv(out_path, index=False)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    # hybrid\n",
    "    try:\n",
    "        if len(df_hybrid) > 0:\n",
    "            df = format_df_diesel_hybrid(df_hybrid, tolerance=tolerance, remove_nan_targets=True)\n",
    "            if len(df_hybrid) > 0:\n",
    "                df = join_weather(df, st_timestamp, et_timestamp)\n",
    "                temp = st_datestr.replace('/', '-')\n",
    "                out_path = os.path.join(HYBRID_PATH, f\"{temp}.csv\")\n",
    "                df.to_csv(out_path, index=False)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # electric\n",
    "    try:\n",
    "        if len(df_electric) > 0:\n",
    "            df = format_df_electric(df_electric, tolerance=tolerance, remove_nan_targets=True)\n",
    "            if len(df_electric) > 0:\n",
    "                df = join_weather(df, st_timestamp, et_timestamp)\n",
    "                temp = st_datestr.replace('/', '-')\n",
    "                out_path = os.path.join(ELECTRIC_PATH, f\"{temp}.csv\")\n",
    "                df.to_csv(out_path, index=False)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    \n",
    "    end_time = time.time() - start_time\n",
    "    #print(f\"Took {end_time} seconds to run day {temp}\")\n",
    "    #print(\"......\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_DATESTR = \"2020/01/24\"\n",
    "#ET_DATESTR = \"2020/02/01\"\n",
    "ET_DATESTR = \"2020/07/01\"\n",
    "NUM_SAMPLES = 0\n",
    "NUM_SAMPLES_MAPPED = 0\n",
    "\n",
    "date_range = pd.date_range(start=ST_DATESTR, end=ET_DATESTR)\n",
    "for i in range(len(date_range)-1):\n",
    "    st_datestr = date_range[i].isoformat().split('T')[0].replace('-', '/')\n",
    "    et_datestr = date_range[i+1].isoformat().split('T')[0].replace('-', '/')\n",
    "    t = main_run_day(st_datestr, et_datestr)\n",
    "print(NUM_SAMPLES, NUM_SAMPLES_MAPPED)\n",
    "print(NUM_SAMPLES_MAPPED / NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-burns",
   "metadata": {},
   "source": [
    "## 2. Generate Trajectories and Training Samples\n",
    "\n",
    "Reads from: output/data-joins\n",
    "\n",
    "Writes to: output/samples, output/traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIESEL_PATH = os.path.join(os.getcwd(), 'output_r', 'data-joins', 'diesel')\n",
    "HYBRID_PATH = os.path.join(os.getcwd(), 'output_r', 'data-joins', 'hybrid')\n",
    "ELECTRIC_PATH = os.path.join(os.getcwd(), 'output_r', 'data-joins', 'electric')\n",
    "IMAGES_DIR = os.path.join(os.getcwd(), 'output_r', 'images')\n",
    "SEGMENTS_PATH = os.path.join(os.getcwd(), 'output_r', 'segments', 'segments.pkl')\n",
    "\n",
    "SAMPLES_DIR = os.path.join(os.getcwd(), 'output_r', 'samples')\n",
    "TRAJ_DIR = os.path.join(os.getcwd(), 'output_r', 'traj')\n",
    "\n",
    "TZ = pytz.timezone('EST')\n",
    "TIMEZONE_STR = \"America/New_York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point(row, pref):\n",
    "    return Point(row[f\"{pref}_stop_lon\"], row[f\"{pref}_stop_lat\"])\n",
    "\n",
    "\n",
    "def format_segments(df, add_stop_points=True, trip_id_format=False):\n",
    "    df = df.set_geometry('geometry')\n",
    "    df['trip_id'] = df['trip_id'].astype(int)\n",
    "    df['segment_seq'] = df['segment_seq'].astype(int)\n",
    "    df['start_stop_id'] = df['start_stop_id'].astype(int)\n",
    "    df['end_stop_id'] = df['end_stop_id'].astype(int)\n",
    "    df['direction_id'] = df['direction_id'].astype(int)\n",
    "    df['route_id'] = df['route_id'].astype(str)\n",
    "    df['distance_btw_stops'] = df['distance_btw_stops'].astype(float)\n",
    "    df['start_stop_name'] = df['start_stop_name'].astype(str)\n",
    "    df['end_stop_name'] = df['end_stop_name'].astype(str)\n",
    "    df['distance_m'] = df['distance_m'].astype(float)\n",
    "    df['gtfs_start_date'] = df['gtfs_start_date'].apply(lambda x: datetime.date.fromisoformat(x))\n",
    "    df['gtfs_end_date'] = df['gtfs_end_date'].apply(lambda x: datetime.date.fromisoformat(x))\n",
    "    df['segment_id'] = df['segment_id'].astype(str)\n",
    "    \n",
    "    df['XDSegID'] = df['XDSegID'].apply(lambda x: apply_format_list(x, col_type='int'))\n",
    "    df['osm_ways'] = df['osm_ways'].apply(lambda x: apply_format_list(x, col_type='int'))\n",
    "    df['tmc_id'] = df['tmc_id'].apply(lambda x: apply_format_list(x, col_type='string'))\n",
    "    df['osm_way_fclasses'] = df['osm_way_fclasses'].apply(lambda x: apply_format_list(x, col_type='string'))\n",
    "    df['elevation_list'] = df['elevation_list'].apply(lambda x: apply_format_list(x, col_type='float'))\n",
    "    \n",
    "    if add_stop_points:\n",
    "        df['start_stop_geometry'] = df.apply(lambda row: get_point(row, 'start'), axis=1)\n",
    "        df['end_stop_geometry'] = df.apply(lambda row: get_point(row, 'end'), axis=1)\n",
    "        df = df.drop(columns=['start_stop_lon', 'start_stop_lat', 'end_stop_lat', 'end_stop_lon'])\n",
    "    else:\n",
    "        df['start_stop_lon'] = df['start_stop_lon'].astype(float)\n",
    "        df['end_stop_lon'] = df['end_stop_lon'].astype(float)\n",
    "        df['start_stop_lat'] = df['start_stop_lat'].astype(float)\n",
    "        df['end_stop_lat'] = df['end_stop_lat'].astype(float)\n",
    "    if trip_id_format:\n",
    "        df['trip_id'] = df['trip_id'].apply(lambda x: int(str(int(x))[0:-3]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def timestamp_to_datetime(timestamp, millis=False):\n",
    "    if millis is True:\n",
    "        ts = timestamp / 1000\n",
    "    else:\n",
    "        ts = timestamp\n",
    "    dt = datetime.datetime.fromtimestamp(ts, tz=TZ)\n",
    "    return dt\n",
    "\n",
    "\n",
    "def format_gps(x):\n",
    "    lat, lon = x.split(\"|\")\n",
    "    return Point(float(lon), float(lat))\n",
    "\n",
    "\n",
    "def load_data_gdf(dir_path, trip_id_format=False):\n",
    "    result = []\n",
    "    files = os.listdir(dir_path)\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            temp = pd.read_csv(os.path.join(dir_path, file), low_memory=False)\n",
    "            result.append(temp.copy(deep=True))\n",
    "            #print(f\"loaded: {file}\")\n",
    "    df = pd.concat(result, ignore_index=True)\n",
    "    df = gpd.GeoDataFrame(df)\n",
    "    df['datetime'] = df['time'].apply(lambda x: timestamp_to_datetime(x, millis=True))\n",
    "    df['timestamp_ms'] = df['time']\n",
    "    df['date'] = df['datetime'].apply(lambda x: x.date())\n",
    "    df['geometry'] = df['gps'].apply(lambda x: format_gps(x))\n",
    "    if trip_id_format:\n",
    "        df['trip_id'] = df['trip_id'].apply(lambda x: str(int(x))[0:-3])\n",
    "    df = df.drop(columns=['time', 'gps'])\n",
    "    df = df.set_geometry('geometry')\n",
    "    df = df.set_crs('EPSG:4326')\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_trips_with_duplicates(df):\n",
    "    print(f\"Number of unique trips: {len(df['trip_id'].unique())}\")\n",
    "    trips_to_keep = []\n",
    "    for trip_id in df['trip_id'].unique():\n",
    "        temp = df[df['trip_id']==trip_id]\n",
    "        if len(temp['start_stop_id'].unique()) == len(temp):\n",
    "            trips_to_keep.append(trip_id)\n",
    "    result = df[df['trip_id'].isin(trips_to_keep)]\n",
    "    print(f\"Number of trips without duplicate stops: {len(result['trip_id'].unique())}\")\n",
    "    return df[df['trip_id'].isin(trips_to_keep)]\n",
    "    \n",
    "    \n",
    "def get_mongo_connection():\n",
    "    with open(CONFIG_PATH) as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    mongo_url = \"mongodb://{}:{}@{}:{}/?authSource={}\".format(config[\"user\"],\n",
    "                                                              config[\"password\"],\n",
    "                                                              config[\"host\"],\n",
    "                                                              config[\"port\"],\n",
    "                                                              config[\"authenticationDatabase\"])\n",
    "\n",
    "    client = pymongo.MongoClient(mongo_url)\n",
    "    return client\n",
    "\n",
    "project = partial(\n",
    "    pyproj.transform,\n",
    "    pyproj.Proj('EPSG:4326'),\n",
    "    pyproj.Proj('EPSG:32616'))\n",
    "    \n",
    "def project_linestring(line1, project):\n",
    "    line1_inv = LineString([(x[1], x[0]) for x in list(line1.coords)])\n",
    "    return transform(project, line1_inv)\n",
    "\n",
    "\n",
    "def project_point(p, project):\n",
    "    p_inv = Point((p.y, p.x))\n",
    "    return transform(project, p_inv)\n",
    "\n",
    "\n",
    "\n",
    "def get_all_trips(df):\n",
    "    trips = df.groupby(['trip_id','clever_vid', 'date']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['date'])\n",
    "    return trips[['trip_id', 'clever_vid', 'date']]\n",
    "\n",
    "\n",
    "def get_data_for_trip(df, trip_id, clever_vid, date):\n",
    "    return df[(df['trip_id']==trip_id) & (df['clever_vid']==clever_vid) & (df['date']==date)].sort_values(by=['timestamp_ms'])\n",
    "\n",
    "\n",
    "def get_segments_for_trip(df_seg, trip_id, date_obj):\n",
    "    return df_seg[(df_seg['trip_id']==trip_id) & (df_seg['gtfs_start_date'] <= date_obj) & (df_seg['gtfs_end_date'] > date_obj)].sort_values('segment_seq')\n",
    "\n",
    "\n",
    "def check_sequences(df_trip_seg):\n",
    "    l = df_trip_seg['segment_seq'].tolist()\n",
    "    for i in range(len(l)):\n",
    "        if l[i] != i:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def mapmatch(df_trip_seg, df_trip_traj, seg_lookahead=SEG_LOOKAHEAD, buffer=BUFFER):\n",
    "    df_trip_traj = df_trip_traj.sort_values(by=['timestamp_ms'])\n",
    "    df_trip_seg = df_trip_seg.sort_values(by=['segment_seq'])\n",
    "    \n",
    "    if check_sequences(df_trip_seg) is False:\n",
    "        print('issue')\n",
    "        return None\n",
    "    \n",
    "    if 'geometry_proj' not in df_trip_seg.columns:\n",
    "        df_trip_seg['geometry_proj'] = df_trip_seg['geometry'].apply(lambda x: project_linestring(x, project))\n",
    "    if 'geometry_proj' not in df_trip_traj.columns:\n",
    "        df_trip_traj['geometry_proj'] = df_trip_traj['geometry'].apply(lambda x: project_point(x, project))\n",
    "        \n",
    "    result_list = []\n",
    "    cur_seg_seq = 0\n",
    "    for traj_index in range(len(df_trip_traj)):\n",
    "        dist_to_segments = []\n",
    "        for i in range(cur_seg_seq, cur_seg_seq + seg_lookahead):\n",
    "            try:\n",
    "                dist_to_segment = df_trip_seg.iloc[i]['geometry_proj'].distance(df_trip_traj.iloc[traj_index]['geometry_proj'])\n",
    "                dist_to_segments.append(dist_to_segment)\n",
    "            except:\n",
    "                pass\n",
    "        best_dist = 100000000\n",
    "        best_new = None\n",
    "        for j in range(len(dist_to_segments)):\n",
    "            if dist_to_segments[j] < best_dist:\n",
    "                best_dist = dist_to_segments[j]\n",
    "                best_new = j\n",
    "        if best_dist <= buffer:\n",
    "            cur_seg_seq += best_new\n",
    "            result_list.append(cur_seg_seq)\n",
    "        else:\n",
    "            result_list.append(None)\n",
    "    df_trip_traj['segment_seq'] = result_list\n",
    "    df_trip_traj = df_trip_traj.dropna(subset=['segment_seq'])\n",
    "    df_trip_traj['segment_seq'] = df_trip_traj['segment_seq'].astype(int)\n",
    "    df_trip_seg['segment_seq'] = df_trip_seg['segment_seq'].astype(int)\n",
    "\n",
    "    df_result_final = pd.merge(left=df_trip_traj, right=df_trip_seg, how='left', on='segment_seq', validate='many_to_one')\n",
    "    df_result_final['gps_geometry'] = df_result_final['geometry_x']\n",
    "    df_result_final['gps_geometry_proj'] = df_result_final['geometry_proj_x']\n",
    "    df_result_final['trip_id'] = df_result_final['trip_id_x']\n",
    "    df_result_final = df_result_final.drop(columns=['trip_id_x', 'trip_id_y', 'geometry_proj_x', 'geometry_proj_y', 'geometry_x', 'geometry_y'])\n",
    "    return df_result_final\n",
    "\n",
    "\n",
    "def generate_training_samples(df, vehicle_type='diesel'):\n",
    "    result_list = []\n",
    "    unique_segments = df['segment_id'].unique()\n",
    "    for segment_id in unique_segments:\n",
    "        df_temp = df[df['segment_id']==segment_id].sort_values(by=['timestamp_ms'])\n",
    "        if len(df_temp) > 1:\n",
    "            first_reading = df_temp.iloc[0]\n",
    "            reading = first_reading.to_dict()\n",
    "            last_reading = df_temp.iloc[-1]\n",
    "            reading['actual_start_elevation'] = request_elevation(first_reading['gps_geometry'].y, first_reading['gps_geometry'].x)\n",
    "            reading['actual_end_elevation'] = request_elevation(last_reading['gps_geometry'].y, last_reading['gps_geometry'].x)\n",
    "            reading['time_diff_ms'] = last_reading['timestamp_ms'] - reading['timestamp_ms']\n",
    "            reading['gps_traj_proj'] = LineString(df_temp['gps_geometry_proj'].tolist())\n",
    "            reading['gps_traj'] = LineString(df_temp['gps_geometry'].tolist())\n",
    "            reading['true_distance_travelled_m'] = reading['gps_traj_proj'].length\n",
    "            reading['number_of_trajectories'] = len(df_temp)\n",
    "            \n",
    "            if vehicle_type == 'electric':\n",
    "                energy_consumed = 0\n",
    "                for i in range(1, len(df_temp)):\n",
    "                    energy_consumed += df_temp.iloc[i]['power'] * ((df_temp.iloc[i]['timestamp_ms'] / 1000) - (df_temp.iloc[i-1]['timestamp_ms'] / 1000))\n",
    "                reading['energy_consumed_kwh'] = energy_consumed / 3600.0\n",
    "            else:\n",
    "                reading['fuel_diff_l'] = last_reading['analyses.fuel_used'] - reading['analyses.fuel_used']\n",
    "            \n",
    "            result_list.append(reading)\n",
    "    df_result = pd.DataFrame(result_list)\n",
    "    if len(df_result) > 0:\n",
    "        if vehicle_type == 'electric':\n",
    "            df_result = df_result.drop(columns=['_id', 'power', 'timezone', 'agency', 'summary', 'icon'])\n",
    "        else:\n",
    "            df_result = df_result.drop(columns=['_id', 'analyses.fuel_used', 'timezone', 'agency', 'summary', 'icon'])\n",
    "        return df_result\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def sample_to_traj(df_mapping, trip_id, date_obj, clever_vid, segment_seq):\n",
    "    return df_mapping[(df_mapping['trip_id']==trip_id) & (df_mapping['date']==date_obj) & (df_mapping['clever_vid']==clever_vid) & (df_mapping['segment_seq']==segment_seq)].sort_values(by=['timestamp_ms'])\n",
    "    \n",
    "    \n",
    "def trip_to_samples(df_samples, trip_id, date_obj, clever_vid):\n",
    "    return df_samples[(df_samples['trip_id']==trip_id) & (df_samples['date']==date_obj) & (df_samples['clever_vid']==clever_vid)].sort_values(by=['segment_seq'])\n",
    "\n",
    "\n",
    "def trip_to_traj(df_mapping, trip_id, date_obj, clever_vid):\n",
    "    return df_mapping[(df_mapping['trip_id']==trip_id) & (df_mapping['date']==date_obj) & (df_mapping['clever_vid']==clever_vid)].sort_values(by=['timestamp_ms'])\n",
    "\n",
    "\n",
    "def request_elevation(lat,\n",
    "                      lon,\n",
    "                      units='Meters',\n",
    "                      max_tries=10,\n",
    "                      sec_btw_tries=1):\n",
    "    usgs_url = r'https://nationalmap.gov/epqs/pqs.php?'\n",
    "    usgs_params = {'output': 'json', 'x': lon, 'y': lat, 'units': units}\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "            usgs_request = requests.get(url=usgs_url,\n",
    "                                        params=usgs_params)\n",
    "            elevation = float(usgs_request.json()['USGS_Elevation_Point_Query_Service']['Elevation_Query']['Elevation'])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            elevation = None\n",
    "            time.sleep(sec_btw_tries)\n",
    "    return elevation\n",
    "\n",
    "\n",
    "def apply_format_list(x, col_type='int'):\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace(\"[\", \"\")\n",
    "        x = x.replace(\"]\", \"\")\n",
    "        x = x.replace(\" \", \"\")\n",
    "        x = x.replace(\"'\", \"\")\n",
    "        if col_type == 'int':\n",
    "            return [int(y) for y in x.split(\",\")]\n",
    "        elif col_type == 'float':\n",
    "            return [float(y) for y in x.split(\",\")]\n",
    "        elif col_type == \"string\":\n",
    "            return [str(y) for y in x.split(\",\")]\n",
    "        else:\n",
    "            return [y for y in x.split(\",\")]\n",
    "            \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_joined(file_path, trip_id_format=False):\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    df = gpd.GeoDataFrame(df)\n",
    "    df['datetime'] = df['time'].apply(lambda x: timestamp_to_datetime(x, millis=True))\n",
    "    df['timestamp_ms'] = df['time']\n",
    "    df['date'] = df['datetime'].apply(lambda x: x.date())\n",
    "    df['geometry'] = df['gps'].apply(lambda x: format_gps(x))\n",
    "    if trip_id_format:\n",
    "        df['trip_id'] = df['trip_id'].apply(lambda x: str(int(x))[0:-3])\n",
    "    df = df.drop(columns=['time', 'gps'])\n",
    "    df = df.set_geometry('geometry')\n",
    "    df = df.set_crs('EPSG:4326')\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_all_trips(df, df_seg, vehicle_type=\"diesel\"):\n",
    "    trips = get_all_trips(df)\n",
    "\n",
    "    results_mapmatch = []\n",
    "    results_training_samples = []\n",
    "    start_time = time.time()\n",
    "    for k, trip in trips.iterrows():\n",
    "        df_trip_traj = get_data_for_trip(df, trip['trip_id'], trip['clever_vid'], trip['date'])\n",
    "        df_trip_seg = get_segments_for_trip(df_seg, trip['trip_id'], trip['date'])\n",
    "        if (len(df_trip_traj) > 3) & (len(df_trip_seg) > 3):\n",
    "            df_test = mapmatch(df_trip_seg, df_trip_traj)\n",
    "            if len(df_test) > 0:\n",
    "                results_mapmatch.append(df_test)\n",
    "                df_result = generate_training_samples(df_test, vehicle_type=vehicle_type)\n",
    "                if df_result is not None:\n",
    "                    if len(df_result) > 0:\n",
    "                        results_training_samples.append(df_result)\n",
    "    try:\n",
    "        df_samples = pd.concat(results_training_samples, ignore_index=True)\n",
    "        df_samples = df_samples.drop(columns=['time_string', 'rt', 'nearest_storm_distance', 'dew_point', 'wind_bearing', 'cloud_cover', 'uv_index', 'ozone', 'distance_m', 'gtfs_start_date', 'gtfs_end_date', 'elevation_list', 'gps_geometry', 'gps_geometry_proj'])\n",
    "        df_samples['speed_meters_per_second'] = df_samples.apply(lambda row: row['true_distance_travelled_m'] / (row['time_diff_ms']/1000), axis=1)\n",
    "        df_traj = pd.concat(results_mapmatch, ignore_index=True)\n",
    "        return df_samples, df_traj\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def process_file(file):\n",
    "    start_time = time.time()\n",
    "    #print(f\"Starting to process {file}\")\n",
    "        \n",
    "    file_path = os.path.join(dir_path, file)\n",
    "    df = load_data_joined(file_path)\n",
    "    df['geometry_proj'] = df.to_crs('EPSG:32616')['geometry']\n",
    "    df_samples, df_traj = process_all_trips(df, DF_SEG.copy(deep=True), vehicle_type=vehicle_type)\n",
    "    if (df_samples is not None) & (df_traj is not None):\n",
    "        file_name_new = file.split(\".\")[0] + \".pkl\"\n",
    "        out_path = os.path.join(SAMPLES_DIR, vehicle_type, file_name_new)\n",
    "        df_samples.to_pickle(out_path)\n",
    "        out_path = os.path.join(TRAJ_DIR, vehicle_type, file_name_new)\n",
    "        df_traj.to_pickle(out_path)\n",
    "        \n",
    "    end_time = time.time() - start_time\n",
    "    #print(f\"Done processing {file} in {end_time} seconds\")\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_SEG = pd.read_pickle(SEGMENTS_PATH)\n",
    "DF_SEG = format_segments(DF_SEG, add_stop_points=True, trip_id_format=True)\n",
    "DF_SEG = remove_trips_with_duplicates(DF_SEG)\n",
    "DF_SEG['geometry_proj'] = DF_SEG.to_crs('EPSG:32616')['geometry']\n",
    "DF_SEG.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_type = \"hybrid\"\n",
    "dir_path = HYBRID_PATH\n",
    "files = os.listdir(dir_path)\n",
    "files.sort()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=30) as executor:\n",
    "    futures = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            futures.append(executor.submit(process_file, file))\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        r = future.result()\n",
    "        #print(r)\n",
    "        \n",
    "print(\"DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_type = \"diesel\"\n",
    "dir_path = DIESEL_PATH\n",
    "files = os.listdir(dir_path)\n",
    "files.sort()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=30) as executor:\n",
    "    futures = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            futures.append(executor.submit(process_file, file))\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        r = future.result()\n",
    "        #print(r)\n",
    "        \n",
    "print(\"DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_type = \"electric\"\n",
    "dir_path = ELECTRIC_PATH\n",
    "files = os.listdir(dir_path)\n",
    "files.sort()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=30) as executor:\n",
    "    futures = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            futures.append(executor.submit(process_file, file))\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        r = future.result()\n",
    "        #print(r)\n",
    "        \n",
    "print(\"DONE!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-heather",
   "metadata": {},
   "source": [
    "## 3. Add Traffic\n",
    "\n",
    "Reads from: output/samples\n",
    "\n",
    "Writes to: output/samples_with_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_DIR = os.path.join(os.getcwd(), 'output_r', 'samples')\n",
    "SAMPLES_WITH_TRAFFIC_DIR = os.path.join(os.getcwd(), 'output_r', 'samples_with_traffic')\n",
    "\n",
    "TZ = pytz.timezone('EST')\n",
    "TIMEZONE_STR = \"America/New_York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mongo_connection():\n",
    "    with open(CONFIG_PATH) as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    mongo_url = \"mongodb://{}:{}@{}:{}/?authSource={}\".format(config[\"user\"],\n",
    "                                                              config[\"password\"],\n",
    "                                                              config[\"host\"],\n",
    "                                                              config[\"port\"],\n",
    "                                                              config[\"authenticationDatabase\"])\n",
    "\n",
    "    client = pymongo.MongoClient(mongo_url)\n",
    "    return client\n",
    "\n",
    "\n",
    "def read_samples(file_path):\n",
    "    return pd.read_pickle(file_path)\n",
    "\n",
    "\n",
    "def speed_ratio(row):\n",
    "    try:\n",
    "        result = row['SU'] / row['FF']\n",
    "    except Exception as e:\n",
    "        result = np.nan\n",
    "    return result\n",
    "\n",
    "\n",
    "def query_ave_here_traffic(client, tmc_ids, timestamp_start, timestamp_end, db=\"data-class\", collection=\"here.chattanooga\"):\n",
    "    query = {'tmc_id': {\"$in\": tmc_ids}, \"time\": {\"$lte\": timestamp_end, \"$gte\": timestamp_start}}\n",
    "    traffic = pd.DataFrame(list(client[db][collection].find(query)))\n",
    "    if len(traffic) >= 1:\n",
    "        traffic['SR'] = traffic.apply(lambda row: speed_ratio(row), axis=1)\n",
    "        return traffic['SR'].mean(), traffic['JF'].mean()\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "def process_file_traffic(file):\n",
    "    client = get_mongo_connection()\n",
    "    start_time = time.time()\n",
    "    file_path = os.path.join(SAMPLES_DIR, vehicle_type, file)\n",
    "    df = pd.read_pickle(file_path)\n",
    "    \n",
    "    sr_nearest_list = []\n",
    "    jf_nearest_list = []\n",
    "    sr_trip_list = []\n",
    "    jf_trip_list = []\n",
    "    \n",
    "    for k, v in df.iterrows():\n",
    "        timestamp_start = int(v['timestamp_ms'] / 1000) - TRAFFIC_BUFFER\n",
    "        timestamp_end = timestamp_start + 2 * TRAFFIC_BUFFER\n",
    "        if isinstance(v['tmc_id'], list):\n",
    "            sr_nearest, jf_nearest = query_ave_here_traffic(client, v['tmc_id'], timestamp_start, timestamp_end)\n",
    "        else:\n",
    "            sr_nearest = np.nan\n",
    "            jf_nearest = np.nan\n",
    "        df_temp = df[(df['trip_id']==v['trip_id']) & (df['clever_vid']==v['clever_vid'])]\n",
    "        tmc_ids = []\n",
    "        for kk, vv in df_temp.iterrows():\n",
    "            if isinstance(vv['tmc_id'], list):\n",
    "                tmc_ids += vv['tmc_id']\n",
    "        if len(tmc_ids) > 1:\n",
    "            sr_trip, jf_trip = query_ave_here_traffic(client, tmc_ids, timestamp_start, timestamp_end)\n",
    "        else:\n",
    "            sr_trip = np.nan\n",
    "            jf_trip = np.nan\n",
    "        sr_nearest_list.append(sr_nearest)\n",
    "        jf_nearest_list.append(jf_nearest)\n",
    "        sr_trip_list.append(sr_trip)\n",
    "        jf_trip_list.append(jf_trip)\n",
    "        \n",
    "    df['sr_ave_segment'] = sr_nearest_list\n",
    "    df['jf_ave_segment'] = jf_nearest_list\n",
    "    df['sr_ave_trip'] = sr_trip_list\n",
    "    df['jf_ave_trip'] = jf_trip_list\n",
    "    \n",
    "    out_path = os.path.join(SAMPLES_WITH_TRAFFIC_DIR, vehicle_type, file)\n",
    "    df.to_pickle(out_path)\n",
    "    \n",
    "    end_time = time.time() - start_time\n",
    "    #print(f\"Done processing {file} in {end_time} seconds\")\n",
    "    client.close()\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_type = 'hybrid'\n",
    "\n",
    "files = os.listdir(os.path.join(SAMPLES_DIR, vehicle_type))\n",
    "files.sort()\n",
    "        \n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=30) as executor:\n",
    "    futures = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".pkl\"):\n",
    "            futures.append(executor.submit(process_file_traffic, file))\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        r = future.result()\n",
    "        #print(r)\n",
    "        \n",
    "print(\"DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_type = 'diesel'\n",
    "\n",
    "files = os.listdir(os.path.join(SAMPLES_DIR, vehicle_type))\n",
    "files.sort()\n",
    "        \n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=30) as executor:\n",
    "    futures = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".pkl\"):\n",
    "            futures.append(executor.submit(process_file_traffic, file))\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        r = future.result()\n",
    "        #print(r)\n",
    "        \n",
    "print(\"DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_type = 'electric'\n",
    "\n",
    "files = os.listdir(os.path.join(SAMPLES_DIR, vehicle_type))\n",
    "files.sort()\n",
    "        \n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    futures = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".pkl\"):\n",
    "            futures.append(executor.submit(process_file_traffic, file))\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        r = future.result()\n",
    "        #print(r)\n",
    "        \n",
    "print(\"DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-lambda",
   "metadata": {},
   "source": [
    "## 4. Generate Training Set - reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_WITH_TRAFFIC_DIR = os.path.join(os.getcwd(), 'output_r', 'samples_with_traffic')\n",
    "TRAINING_DIR = os.path.join(os.getcwd(), 'output_r', 'training')\n",
    "\n",
    "TZ = pytz.timezone('EST')\n",
    "TIMEZONE_STR = \"America/New_York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_set(vehicle_type, add_date=False):\n",
    "    files = os.listdir(os.path.join(SAMPLES_WITH_TRAFFIC_DIR, vehicle_type))\n",
    "    files.sort()\n",
    "    \n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".pkl\"):\n",
    "            file_path = os.path.join(SAMPLES_WITH_TRAFFIC_DIR, vehicle_type, file)\n",
    "            df = read_samples(file_path)\n",
    "            if add_date:\n",
    "                df['date'] = file.split(\".\")[0]\n",
    "            df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    print(len(df))\n",
    "    df = df.drop(['precipitation_probability', 'apparent_temperature', 'start_stop_id', 'end_stop_id', 'shape_id', 'direction_id', 'start_stop_name', 'end_stop_name', 'start_stop_geometry', 'end_stop_geometry', 'gps_traj_proj', 'gps_traj'], axis=1)\n",
    "    df['time_diff_s'] = df['time_diff_ms'].apply(lambda x: int(x/1000))\n",
    "    df['timestamp_s'] = df['timestamp_ms'].apply(lambda x: int(x/1000))\n",
    "    df['actual_elevation_change'] = df.apply(lambda row: row['actual_end_elevation'] - row['actual_start_elevation'], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def one_hot_encode_roadway(df):\n",
    "    unique_fclasses = set()\n",
    "    for k, v in df.iterrows():\n",
    "        if isinstance(v['osm_way_fclasses'], list):\n",
    "            for x in v['osm_way_fclasses']:\n",
    "                if x == '':\n",
    "                    pass\n",
    "                else:\n",
    "                    unique_fclasses.add(x)\n",
    "\n",
    "    unique_fclasses = list(unique_fclasses)\n",
    "    print(unique_fclasses)\n",
    "    result = {key: [] for key in unique_fclasses}\n",
    "\n",
    "    for k, v in df.iterrows():\n",
    "        if isinstance(v['osm_way_fclasses'], list):\n",
    "            for fclass in unique_fclasses:\n",
    "                if fclass in v['osm_way_fclasses']:\n",
    "                    result[fclass].append(1)\n",
    "                else:\n",
    "                    result[fclass].append(0)\n",
    "        else:\n",
    "            for fclass in unique_fclasses:\n",
    "                result[fclass].append(0)\n",
    "\n",
    "    for k, v in result.items():\n",
    "        df[k] = v\n",
    "\n",
    "    df['distance_travelled_m'] = df['true_distance_travelled_m']\n",
    "    df['distance_btw_stops_m'] = df['distance_btw_stops']\n",
    "    df = df.drop(['time_diff_ms', 'timestamp_ms', 'osm_way_fclasses', 'true_distance_travelled_m', 'distance_btw_stops'], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def trim_by_percentile(df, target, cutoff):\n",
    "    lower = df[target].quantile(cutoff)\n",
    "    temp = 1 - cutoff\n",
    "    upper = df[target].quantile(temp)\n",
    "    df = df[(df[target]>=lower) & (df[target]<=upper)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_fun(row, lower_bound, upper_bound):\n",
    "    if (row['distance_travelled_m'] >= lower_bound * row['distance_btw_stops_m']) and (row['distance_travelled_m'] <= upper_bound * row['distance_btw_stops_m']):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def filt_by_traj_dist(df, lower_bound, upper_bound, min_dist):\n",
    "    df['filt'] = df.apply(lambda row: apply_fun(row, lower_bound, upper_bound), axis=1) \n",
    "    df = df[df['filt']==1]\n",
    "    df = df[df['distance_travelled_m']>min_dist]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_emmisions(df, vehicle_type):\n",
    "    if vehicle_type == \"electric\":\n",
    "        df['target_kg'] = df['energy_consumed_kwh'].apply(lambda x: x * 0.707)\n",
    "        df['target_kg_per_km'] = df.apply(lambda row: (row['target_kg'] * 1000) / row['distance_travelled_m'], axis=1)\n",
    "    else:\n",
    "        df['target_kg'] = df['fuel_diff_l'].apply(lambda x: x * 10.18 / 3.78541)\n",
    "        df['target_kg_per_km'] = df.apply(lambda row: (row['target_kg'] * 1000) / row['distance_travelled_m'], axis=1) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_types = ['electric', 'hybrid', 'diesel']\n",
    "vehicle_type = 'electric'\n",
    "LOWER_BOUND = 0.5\n",
    "UPPER_BOUND = 1.5\n",
    "MIN_DIST = 10\n",
    "CUTOFF = 0.05\n",
    "TARGET = 'target_kg_per_km' # alternatively could be target_kg\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for vehicle_type in vehicle_types:\n",
    "    print(\"........\")\n",
    "    print(vehicle_type)\n",
    "    df = get_training_set(vehicle_type, add_date=False)\n",
    "    print(f\"Total number of samples: {len(df)}\")\n",
    "    df = one_hot_encode_roadway(df)\n",
    "    df = filt_by_traj_dist(df, LOWER_BOUND, UPPER_BOUND, MIN_DIST)\n",
    "    print(f\"Total after distance filtering {len(df)}\")\n",
    "    df = add_emmisions(df, vehicle_type)\n",
    "    if vehicle_type != 'electric':\n",
    "        df = df[df['target_kg'] > 0]\n",
    "    df = trim_by_percentile(df, TARGET, CUTOFF)\n",
    "    print(f\"Total after trimming percentile: {len(df)}\")\n",
    "    df['vehicle_model'] = df['vehicle_type']\n",
    "    df['vehicle_class'] = vehicle_type\n",
    "    df = df.drop(columns=['vehicle_fleet', 'vehicle_type'])\n",
    "    df_list.append(df.copy(deep=True))\n",
    "    print(f\"Final total: {len(df)}\")\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df = df.drop(columns='fuel_diff_l')\n",
    "#file_path = os.path.join('output_r', 'training', 'data2.pkl')\n",
    "#df.to_pickle(file_path)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(file_path)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-basics",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_types = ['electric', 'diesel', 'hybrid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-joins\n",
    "\n",
    "result = 0\n",
    "dir_path = os.path.join(os.getcwd(), 'output_r', 'data-joins')\n",
    "for vehicle_type in vehicle_types:\n",
    "    dir_path_vehicle = os.path.join(dir_path, vehicle_type)\n",
    "    for file in os.listdir(dir_path_vehicle):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(dir_path_vehicle, file)\n",
    "            df = load_data_joined(file_path)\n",
    "            result += len(df)\n",
    "print(f\"Total number of GPS readings joined: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapmatch results\n",
    "\n",
    "result = 0\n",
    "dir_path = os.path.join(os.getcwd(), 'output_r', 'traj')\n",
    "for vehicle_type in vehicle_types:\n",
    "    dir_path_vehicle = os.path.join(dir_path, vehicle_type)\n",
    "    for file in os.listdir(dir_path_vehicle):\n",
    "        if file.endswith(\".pkl\"):\n",
    "            file_path = os.path.join(dir_path_vehicle, file)\n",
    "            df = pd.read_pickle(file_path)\n",
    "            result += len(df)\n",
    "print(f\"Total number of GPS readings mapped to segments: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "\n",
    "result = 0\n",
    "dir_path = os.path.join(os.getcwd(), 'output_r', 'samples')\n",
    "for vehicle_type in vehicle_types:\n",
    "    dir_path_vehicle = os.path.join(dir_path, vehicle_type)\n",
    "    for file in os.listdir(dir_path_vehicle):\n",
    "        if file.endswith(\".pkl\"):\n",
    "            file_path = os.path.join(dir_path_vehicle, file)\n",
    "            df = pd.read_pickle(file_path)\n",
    "            result += len(df)\n",
    "print(f\"Total number of samples available for training: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples with traffic\n",
    "\n",
    "result = 0\n",
    "dir_path = os.path.join(os.getcwd(), 'output_r', 'samples_with_traffic')\n",
    "for vehicle_type in vehicle_types:\n",
    "    dir_path_vehicle = os.path.join(dir_path, vehicle_type)\n",
    "    for file in os.listdir(dir_path_vehicle):\n",
    "        if file.endswith(\".pkl\"):\n",
    "            file_path = os.path.join(dir_path_vehicle, file)\n",
    "            df = pd.read_pickle(file_path)\n",
    "            result += len(df)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-finding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
